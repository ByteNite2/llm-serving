{
  "name": "llama4-app-gpu",
  "version": "1.0",
  "platform": "docker",
  "platform_config": {
    "container": "chandrabytenite/llama4-scout-gpu:v0.4"
  },
  "entrypoint": "main.py",
  "device_requirements": {
    "min_cpu": 2,
    "min_memory": 2,
    "gpu": [
      "nvidia a100-sxm4-40gb"
    ]
  },
  "description": "An app named llama4-app-gpu with model stored in cache directory"
}